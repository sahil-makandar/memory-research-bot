# Memory-Persistent Research Assistant

LLM-powered research assistant with function calling, memory persistence, and document indexing.

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start API Server
```bash
python app.py
```
API runs on: http://localhost:8000

### 3. Start UI (New Terminal)
```bash
streamlit run ui.py
```
UI runs on: http://localhost:8501

## Architecture

### LLM Function Calling
- Every query processed by LLM
- LLM decides which functions to call:
  - `analyze_query_complexity`
  - `get_memory_context` 
  - `search_adobe_data`
  - `search_documents`
- No hardcoded responses

### Memory System
- Short-term: FIFO conversation buffer
- Long-term: Fact extraction and storage
- Session persistence across queries

### Document Processing
- Upload PDF/TXT files via UI
- Automatic indexing and search
- Context integration in responses

## Features

- **Pure LLM Processing**: All responses generated by LLM
- **Function Calling**: Dynamic data retrieval based on query
- **Memory Persistence**: Conversation context and facts
- **Document Upload**: Index and search capabilities
- **Thinking Process**: Real-time processing steps display

## Testing

```bash
python demo.py          # Feature demonstration
python test_runner.py   # Run test suite
```

## Docker Deployment

```bash
docker-compose up
```